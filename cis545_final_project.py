# -*- coding: utf-8 -*-
"""CIS545 Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FOU86nhxgZ3Wn8t1ti_35iv129hI3SM4

# United States College Rankings
Author: Alan Qiao, John-Wesley Appleton

# Introduction
In this notebook, we intend on developing a model to predict the admissions rate of colleges based on empirical data collected from the US Department of Education.college.

Our baseline model is the admissions rate that all institutions must report. 

Since admission rate is continuous, we used several continuous regression models (Linear, Ridge, Lasso) to construct our model.
"""

import json
import glob
import pandas as pd
import numpy as np
import datetime as dt
import re
import os
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm
from google.colab import drive

"""## 1. Read In Dataset"""

from google.colab import drive
drive.mount('/content/drive')

base_url = '/content/drive/My Drive/Dataset/'

info_df = pd.concat([pd.read_csv(base_url+'MERGED2019_20_PP.csv'),
                     pd.read_csv(base_url+'MERGED2018_19_PP.csv'),
                     pd.read_csv(base_url+'MERGED2017_18_PP.csv'),
                     pd.read_csv(base_url+'MERGED2016_17_PP.csv'),
                     pd.read_csv(base_url+'MERGED2015_16_PP.csv')],
                    keys=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]
                    )
info_df = info_df.reset_index()

info_df

info_df = info_df.drop(columns=['level_1'])
info_df = info_df.rename(columns={'level_0': 'YEAR'})

info_df.head()

info_df.dtypes.value_counts()

info_df.shape

info_df.info(verbose=True, show_counts=True)

info_df = info_df.apply(lambda x: pd.to_numeric(x, errors='coerce'))

"""# 2. Clean Dataset

Since we want to plot admissions rate. We can't keep any data without an admissions rate.
"""

info_df = info_df.dropna(subset=['ADM_RATE'])

"""Our dataset is very large and there likely are many missing values. The cause for this may be that earlier surveys did track these fields, or that obtaining data is highly difficult due to logistic or privacy constraints, or that very few schools or students have this characteristic. Additionally, this could be due to the issue raised above: some data is for multiple years after graduation, and we do not have access to that.
Let's match these fields against the data dictionary to sample what type of fields are missing the most data.
"""

data_dict = pd.read_excel(base_url+'CollegeScorecardDataDictionary.xlsx', sheet_name="Institution_Data_Dictionary")
data_dict.head()

"""In the data set there is a column called "CURROPER" which indicates whether the institution is still in operation. This is not relevant for our analysis, and so we will drop any rows which have 0 in the columns, and then we will drop the column."""

info_df = info_df[info_df['CURROPER'] != 0].drop(columns='CURROPER')
curr_schools = info_df[info_df['YEAR'] == 2019]['UNITID']
info_df = info_df[info_df['UNITID'].isin(curr_schools)]

"""We will first create two percent missing dataframe. The first one contains all of the years for which we have data. The second contains data for only 2019. The reason for the second data frame is that there is some data which is unavailable to us, such as: "DEATH_YR6_RT"


"""

percent_missing = info_df.isnull().sum() * 100 / len(info_df)
missing_value_df = pd.DataFrame({'column_name': info_df.columns,
                                 'percent_missing': percent_missing}).reset_index(drop=True)

percent_missing_2019 = info_df[info_df['YEAR'] == 2019].isnull().sum() * 100 / len(info_df[info_df['YEAR'] == 2019])
missing_value_2019_df = pd.DataFrame({'column_name': info_df[info_df['YEAR'] == 2019].columns,
                                 'percent_missing': percent_missing_2019}).reset_index(drop=True)

missing_value_2019_df

plt.boxplot(missing_value_2019_df['percent_missing'])
plt.show()

plt.hist(missing_value_2019_df['percent_missing'])
plt.show()

"""Notice that the data dictionary has duplicate entries for data elements with more than one value that needs to be specified. For the purpose of joining data element name and the corresponding notes, we only want one entry for each element."""

missing_value_df = missing_value_df.merge(data_dict.drop_duplicates('NAME OF DATA ELEMENT')[['NAME OF DATA ELEMENT', 'VARIABLE NAME', 'NOTES']], how='inner', left_on='column_name', right_on='VARIABLE NAME', suffixes=[None, ''])
missing_value_df = missing_value_df.drop(columns=['VARIABLE NAME'])

missing_value_2019_df = missing_value_2019_df.merge(data_dict.drop_duplicates('NAME OF DATA ELEMENT')[['NAME OF DATA ELEMENT', 'VARIABLE NAME', 'NOTES']], how='inner', left_on='column_name', right_on='VARIABLE NAME', suffixes=[None, ''])
missing_value_2019_df = missing_value_2019_df.drop(columns=['VARIABLE NAME'])
missing_value_2019_df

"""From a quick glance, it seems that some columns with 100% data missing are discontinued. These we can safely drop, so we first add them to a list of columns to_drop before we examine the remaining more closely."""

columns_to_drop = missing_value_df[missing_value_df['NOTES'].str.contains('Discontinued', na=False)]['column_name']
columns_to_drop

"""That's 306 taken care of. Lots more to go."""

info_df = info_df.drop(columns=columns_to_drop)
missing_value_df = missing_value_df[~missing_value_df['NOTES'].str.contains('Discontinued', na=False)]

missing_value_2019_df = missing_value_2019_df[~missing_value_2019_df['NOTES'].str.contains('Discontinued', na=False)]
missing_value_2019_df

"""After thoroughly reading the descriptions, it seems that the columns with significant missing values can be placed into four categories.
1. Data Elements that would be very useful to have but unfortunately too sparce to use.  
    a. Examples include fields related to outcome of Pell Grant or First Generation students after leaving institution within 8 years of entry. Knowing more about the outcomes of Pell Grant, First Generation, and part-time students would be helpful to gauge how well the institution can assist special student cohorts, but unfortunately this data is not available perhaps due to difficulty of tracking, or perhaps most smaller institutions simply don't have these populations.
2. Data Elements missing many entries, but a fixed value can be assigned to the missing entries.  
    a. Fields like flag for Historically Black Colleges and Universities have a lot of missing entries, but the missing entries can be assigned value of 0 (No) because all the officially recognized ones have indicated 1 (Yes).
3. Data Elements missing somewhat significant amount of entries, but considering dataset size removing all rows with missing values can be an option.  
    a. These are fields that are sufficiently important that discarding 30% of nearly 50,000 entries could be justified. 35,000 entries is still a rather large dataset, and so training a model on it may still be reasonable. However, caution is needed as if there are multiple such fields and the subset of institutions missing these fields do not overlap, there may be too few data remaining.  
    b. Examples include Cost of Attendance, Proportion of Full-time Faculty
4. Data Elements that may or may not have significant missing entries but can be omitted from analysis.  
    a. An example would be percentage of degrees awarded in a certain field. It does not really make sense to say, for example, that a school offering more Computer Science degrees is better. It may just happen that this school has good resources to offer and also specializes in Computer Science. Thus, dropping these fields is fine regardless of how much data is available.
"""

plt.boxplot(missing_value_2019_df['percent_missing'])
plt.show()

plt.hist(missing_value_2019_df['percent_missing'])
plt.show()

"""Still a lot of columns with close to 100% missing. Let's see what's going on."""

missing_value_2019_df.sort_values(by='percent_missing', ascending=False).head(10)

"""It seems like the remaining columns with 100% missing are columns that are useless to us because not enough time has passed. Let's get rid of them."""

columns_to_drop = missing_value_2019_df[missing_value_2019_df['percent_missing'] == 100]['column_name']
columns_to_drop

"""Wow that's a lot of columns!"""

info_df = info_df.drop(columns=columns_to_drop)
missing_value_df = missing_value_df[missing_value_df['percent_missing'] != 100].sort_values(by='percent_missing', ascending=False)
missing_value_2019_df = missing_value_2019_df[missing_value_2019_df['percent_missing'] != 100].sort_values(by='percent_missing', ascending=False)
missing_value_2019_df.head(10)

missing_value_2019_df = missing_value_2019_df.sort_values(by='percent_missing', ascending=False)
missing_value_2019_df.head(10)

plt.boxplot(missing_value_2019_df['percent_missing'])
plt.show()

plt.hist(missing_value_2019_df['percent_missing'])
plt.show()

"""That's looking a lot better! We're in a much better position now. Let's tackle the columns which contain a Boolean value.

Some of these have NaN instead of 0, and so we will replace those. Additionally, some of the columns are only defined for 2019, and so we need to distribute those values down to previous years.

One observation is that any column that is a Flag falls in category 2, as if the flag does not apply, we can simply set it to 0 (no). We first identify these flags.
"""

columns_to_fill = missing_value_df[missing_value_df['NAME OF DATA ELEMENT'].str.contains('Flag')]
columns_to_fill

"""The first 9 are all flags for special institutions. Upon reviewing the changelog for the data set, it seems that these flags were just added in 2019-2020."""

info_df[info_df['YEAR'] == 2019][['TRIBAL', 'PBI', 'NANTI', 'HSI', 'AANAPII', 'HBCU', 'ANNHI', 'WOMENONLY', 'MENONLY']].apply(pd.Series.value_counts)

info_df[info_df['YEAR']==2019]['UNITID'].size

"""In fact, there are a few missing entry in the year that these columns were implemented.  
Since these attributes are unlikely to change past 10 years, we will simply fill in missing entries in the previous years using the value from 2019.
"""

def copyOver(x, year, column, default):
  tmp = info_df.loc[(info_df['YEAR'] == year) & (info_df['UNITID'] == x), [column]]
  if tmp.size < 0:
    return tmp.append(default)
  return tmp

cols = ['TRIBAL', 'PBI', 'NANTI', 'HSI', 'AANAPII', 'HBCU', 'ANNHI', 'WOMENONLY', 'MENONLY']
for c in cols:
  info_df[c] = info_df[c].fillna(0)
  info_df[c] = info_df['UNITID'].apply(lambda x: copyOver(x, 2019, c, 0))

info_df = info_df.fillna(value={'DISTANCEONLY': 0})

percent_missing = info_df.isnull().sum() * 100 / len(info_df)
missing_value_df = pd.DataFrame({'column_name': info_df.columns,
                                 'percent_missing': percent_missing}).reset_index(drop=True)

percent_missing_2019 = info_df[info_df['YEAR'] == 2019].isnull().sum() * 100 / len(info_df[info_df['YEAR'] == 2019])
missing_value_2019_df = pd.DataFrame({'column_name': info_df[info_df['YEAR'] == 2019].columns,
                                 'percent_missing': percent_missing_2019}).reset_index(drop=True)

missing_value_df = missing_value_df.merge(data_dict.drop_duplicates('NAME OF DATA ELEMENT')[['NAME OF DATA ELEMENT', 'VARIABLE NAME', 'NOTES']], how='inner', left_on='column_name', right_on='VARIABLE NAME', suffixes=[None, ''])
missing_value_df = missing_value_df.drop(columns=['VARIABLE NAME'])

missing_value_2019_df = missing_value_2019_df.merge(data_dict.drop_duplicates('NAME OF DATA ELEMENT')[['NAME OF DATA ELEMENT', 'VARIABLE NAME', 'NOTES']], how='inner', left_on='column_name', right_on='VARIABLE NAME', suffixes=[None, ''])
missing_value_2019_df = missing_value_2019_df.drop(columns=['VARIABLE NAME'])

columns_to_fill = missing_value_df[missing_value_df['NAME OF DATA ELEMENT'].str.contains('Flag')]
columns_to_fill

"""Awesome! It's working!

Next, we will remove all the columns with more than 40% missing, because we cannot use KNN on a column with so much missing data.
"""

columns_to_drop = missing_value_df[missing_value_df['percent_missing'] > 40]['column_name']

info_df = info_df.drop(columns=columns_to_drop)

missing_value_df = missing_value_df[missing_value_df['percent_missing'] < 40]

plt.boxplot(missing_value_df['percent_missing'])
plt.show()

plt.hist(missing_value_df['percent_missing'])
plt.show()

"""Finally, we have done all the custom data engineering we could. The final step to cleaning the dataset would be to fill in the missing values with KNN. For this, we will be using the KNNImputer from Sklearn. Since this can only be used on a sklearn dataset, we will save this step until the actual modeling.

# Visualizations

Our goal is to model admission rate and determine which factors contribute the most to a low/high admission rate. This will help us determine potentially what features makes a school more popular and more competitive than others.

One possible factor that may have a strong correlation is tuition. It seems reasonable that schools which are more competitive and thus likely offer better educational outcomes will be able to charge more. So we first try to visualize the correlation between the two.
"""

info_df.info(verbose=True)

plt.scatter(info_df['COSTT4_A'], info_df['ADM_RATE']*100)
plt.ylabel('Admission Rate (%)')
plt.xlabel('Average Cost of Attendance')
plt.title('Average Cost of Attendance Against Admission Rate')
plt.show()

"""There does not seem to be a direct correlation between the two. However, one thing is that as the cost of attendance increases (above $60k), there seems to be an increasingly evident inverse relation between the two. """

plt.scatter(info_df['SATVRMID']+info_df['SATMTMID'], info_df['ADM_RATE']*100)
plt.ylabel('Admission Rate (%)')
plt.xlabel('SAT Midpoint Score')
plt.title('SAT Midpoint Score Against Admission Rate')
plt.show()

"""This time, there is a more evident trend that higher SAT Midpoint scores is correlated with lower Admission rates.

We can perform a similar analysis with the ACT scores, and they should show the same trend.
"""

plt.scatter(info_df['ACTCMMID'], info_df['ADM_RATE']*100)
plt.ylabel('Admission Rate (%)')
plt.xlabel('ACT Midpoint Score')
plt.title('ACT Midpoint Score Against Admission Rate')
plt.show()

"""As expected, we see a similar trend.

One final paramter we want to study is the Percentage of students who received Pell Grants, as these are usually given to poorer students who are performing well academically.
"""

plt.scatter(info_df['PCTPELL'], info_df['ADM_RATE']*100)
plt.ylabel('Admission Rate (%)')
plt.xlabel('Percentage Received Pell Grant')
plt.title('Pell Grant Recipient Rate Against Admission Rate')
plt.show()

"""There seems to be no real correlation between the two.

Finally, let's look for correlation between all our features, just to make sure that there is nothing that will interfere with our regressions.
"""

target = info_df['ADM_RATE']
features_df = info_df.drop(columns=['ADM_RATE'])

features_df = features_df.select_dtypes(exclude=['object'])

import seaborn as sns
# TO-DO: Get the correlation matrix
corr_matrix = features_df.corr()

# TO-DO: Plot correlation heatmap (4 points)
plt.figure(figsize=(30, 30))
sns.heatmap(corr_matrix, vmin=-1, vmax=1, cmap='RdBu')
plt.show()

"""We can see that there are a few blocks of high correlation. For example, the SAT and ACT scores seems highly correlated. This makes sense because a school will have similar requirements for both SAT and ACT, and the score ranges should not be too big, which makes the numbers quite similar.

# Modeling

Now, it is time to fill in the remaining missing data using KNN.
"""

from sklearn.impute import KNNImputer

imp = KNNImputer()
features = imp.fit_transform(features_df)

"""Next, we will split the data in a 80/20 train-test split."""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

"""Since we have many columns, we will first run PCA to compute the explained variance of each component, and reduce the number of components needed for our regression accordingly. We will use 80% Total variance as our threshold."""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
scaler = StandardScaler()
x_train_sc = scaler.fit_transform(x_train)
x_test_sc = scaler.transform(x_test)
pca = PCA(n_components=493)
pca_x_train = pca.fit_transform(x_train_sc)

explained_variance_ratios = pca.explained_variance_ratio_
cum_evr = pca.explained_variance_ratio_.cumsum()
cum_evr

"""We will plot the Explained variance Ratio against Number of Components. For comparison, a line marking 80% is included."""

plt.plot(np.arange(1, 494), cum_evr)
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Cumulative Explained Variance Ratio by Number of Components')
plt.plot(np.arange(1, 494), [0.8]*493)
plt.legend(['cum_evr', '80% total var'])
plt.show()

"""It seems that that we only need 120 components. We transform the train and test set accordingly below."""

pca = PCA(n_components=120)
pca.fit(x_train_sc)
x_train_pca = pca.transform(x_train_sc)
x_test_pca = pca.transform(x_test_sc)

"""## Linear Regression
For our first model, we will attempt a linear regression.


"""

from sklearn.linear_model import LinearRegression

# Your code here
lr = LinearRegression()
lr.fit(x_train_pca, y_train)

y_pred = lr.predict(x_test_pca)
score = lr.score(x_test_pca, y_test)
print(score)

"""Our reported score is 55%. Which is acceptable but not very good.

That did not work too well. Let's try a Lasso Regression next.
"""

from sklearn.linear_model import Lasso

# TO-DO: Instantiate, fit, predict, and score logistic regression
pca_las = Lasso(alpha=1.0)
pca_las.fit(x_train_pca, y_train)
pca_las_y_pred = pca_las.predict(x_test_pca)
test_accuracy = pca_las.score(x_test_pca, y_test)
test_accuracy

"""This is terrible. Clearly, this not the regularization we want. So we should lean towards the other direction and use a Ridge Regression."""

from sklearn.linear_model import Ridge

# Your code here
rr = Ridge()
rr.fit(x_train_pca, y_train)
ridge_score = rr.score(x_test_pca, y_test)
print(ridge_score)

"""Again, we did not get much improvement, it seems that this is the best we can do.

# Conclusions

We were able to generate a linear regression model which we can use to predict the admission rate of a university given data collected as specified in the College ScoreCard data dictionary. While the model accuracy could be improved, at least we can get a general idea of the difficulty of getting into a school. Similarly, this can also be applied to offer us insight on what makes a school competitive. By examining the features that contribute the most to our regression, we can see how to make schools more fair, and give more students opportunities to enroll in the institutions that are right for them.

We faced a lot of challenges given the size and quality of our data. There was a lot of missing values, data was inconsistent between years, some columns contained multiple data types, and many more challenges. We found it difficult to reduce out dataset to a usable quality.

Next steps would be to scrape data from US News. It would be cool to use their ranking as our target, and try to use our features to predict the ranking.
"""